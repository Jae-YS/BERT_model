{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNJn/JVtPi8817Oe7UvEbin",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28c9b1de06324ca282bb7e4a24b73da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8c65de4f5cb42878e3cc08795a27108",
              "IPY_MODEL_5a93c5d47d8944378e5d2af1f5093fc2",
              "IPY_MODEL_3ac1a57221cc47e4bc7b52b92e32f219"
            ],
            "layout": "IPY_MODEL_cb634eef7cd34c178722034a4a1911cb"
          }
        },
        "f8c65de4f5cb42878e3cc08795a27108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92fed82e627b4b139ef33c7c5ab65f2f",
            "placeholder": "​",
            "style": "IPY_MODEL_93169b9e2e4645d9b9a1d28340cacecb",
            "value": "Tokenizing SQuAD: 100%"
          }
        },
        "5a93c5d47d8944378e5d2af1f5093fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75e3a828af4c4c95aef8b7a7bb957ad1",
            "max": 10570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abbb4bb357f54d62ac18ac8aedad0274",
            "value": 10570
          }
        },
        "3ac1a57221cc47e4bc7b52b92e32f219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed4671525e914f65a62e7fc033966962",
            "placeholder": "​",
            "style": "IPY_MODEL_1d7b1302194c4831a06895884b6d6f1a",
            "value": " 10570/10570 [00:05&lt;00:00, 1862.71 examples/s]"
          }
        },
        "cb634eef7cd34c178722034a4a1911cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92fed82e627b4b139ef33c7c5ab65f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93169b9e2e4645d9b9a1d28340cacecb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75e3a828af4c4c95aef8b7a7bb957ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abbb4bb357f54d62ac18ac8aedad0274": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed4671525e914f65a62e7fc033966962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d7b1302194c4831a06895884b6d6f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jae-YS/BERT_model/blob/main/Fine_tune_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SMBW0OImElq4",
        "outputId": "efb06e71-9145-46b5-867b-729f82c1e71d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.11/dist-packages (2.2.1)\n",
            "Collecting transformers==4.40.0\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets==2.16.0 in /usr/local/lib/python3.11/dist-packages (2.16.0)\n",
            "Requirement already satisfied: evaluate==0.4.0 in /usr/local/lib/python3.11/dist-packages (0.4.0)\n",
            "Requirement already satisfied: accelerate==0.32.0 in /usr/local/lib/python3.11/dist-packages (0.32.0)\n",
            "Requirement already satisfied: peft==0.10.0 in /usr/local/lib/python3.11/dist-packages (0.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (2023.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.1) (2.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.32.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.0)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.0) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (0.7)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.0) (3.11.15)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.11/dist-packages (from evaluate==0.4.0) (0.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.32.0) (5.9.5)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1) (12.5.82)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.0) (1.20.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.1) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.16.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.16.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.16.0) (2025.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.1) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.0) (1.17.0)\n",
            "Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m125.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.19.1 transformers-4.40.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "f3783f9813264a3e92d90bbecee7cfe5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install torch==2.2.1 transformers==4.40.0 datasets==2.16.0 evaluate==0.4.0 accelerate==0.32.0 peft==0.10.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SQuAD v1 dataset\n",
        "dataset = load_dataset(\"squad\")"
      ],
      "metadata": {
        "id": "rPLb7CQhErhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering, BertTokenizerFast\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer from Hugging Face\n",
        "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Preprocessing Function: Converts raw examples into tokenized features\n",
        "def preprocess_function(examples):\n",
        "    # Strip whitespace from questions and contexts\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    contexts = [c.strip() for c in examples[\"context\"]]\n",
        "\n",
        "    # Tokenize the QA pairs with chunking for long contexts\n",
        "    tokenized_inputs = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=384,\n",
        "        truncation=\"only_second\",              # Only truncate the context (not the question)\n",
        "        stride=128,                            # Use a stride for overlapping chunks\n",
        "        return_overflowing_tokens=True,        # Enable splitting of long contexts\n",
        "        return_offsets_mapping=True,           # Keep mapping from tokens to character positions\n",
        "        padding=\"max_length\",                  # Pad all sequences to max_length\n",
        "    )\n",
        "\n",
        "    # Track which original example maps to each tokenized chunk\n",
        "    sample_map = tokenized_inputs[\"overflow_to_sample_mapping\"]\n",
        "    offset_mapping = tokenized_inputs[\"offset_mapping\"]\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    example_ids = []\n",
        "    updated_offset_mapping = []\n",
        "\n",
        "    # For each tokenized chunk\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        input_ids = tokenized_inputs[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "        sequence_ids = tokenized_inputs.sequence_ids(i)\n",
        "\n",
        "        # Map back to original sample\n",
        "        sample_index = sample_map[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        example_ids.append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # Adjust offset mapping: only keep offsets for context tokens\n",
        "        updated_offsets = [\n",
        "            o if sequence_ids[k] == 1 else None\n",
        "            for k, o in enumerate(offsets)\n",
        "        ]\n",
        "        updated_offset_mapping.append(updated_offsets)\n",
        "\n",
        "        # If no answer, point to [CLS]\n",
        "        if len(answers[\"answer_start\"]) == 0 or answers[\"answer_start\"][0] is None:\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "            continue\n",
        "\n",
        "        # Character-level start/end positions\n",
        "        start_char = answers[\"answer_start\"][0]\n",
        "        end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "        # Find token start index in context\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "\n",
        "        # Find token end index in context\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        # If answer not fully in chunk, set to [CLS]\n",
        "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "            start_positions.append(cls_index)\n",
        "            end_positions.append(cls_index)\n",
        "        else:\n",
        "            # Find token index that contains the answer's start character\n",
        "            for idx in range(token_start_index, token_end_index + 1):\n",
        "                if offsets[idx][0] <= start_char < offsets[idx][1]:\n",
        "                    start_positions.append(idx)\n",
        "                    break\n",
        "            else:\n",
        "                start_positions.append(cls_index)\n",
        "\n",
        "            # Find token index that contains the answer's end character\n",
        "            for idx in range(token_end_index, token_start_index - 1, -1):\n",
        "                if offsets[idx][0] < end_char <= offsets[idx][1]:\n",
        "                    end_positions.append(idx)\n",
        "                    break\n",
        "            else:\n",
        "                end_positions.append(cls_index)\n",
        "\n",
        "    # Add start/end labels and mapping for use in training and evaluation\n",
        "    tokenized_inputs[\"start_positions\"] = start_positions\n",
        "    tokenized_inputs[\"end_positions\"] = end_positions\n",
        "    tokenized_inputs[\"example_id\"] = example_ids\n",
        "    tokenized_inputs[\"offset_mapping\"] = updated_offset_mapping\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply Preprocessing to Entire Dataset\n",
        "tokenized_datasets = dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,                                         # Apply preprocessing to batches\n",
        "    remove_columns=dataset[\"train\"].column_names,         # Remove original columns to avoid conflicts\n",
        "    desc=\"Tokenizing SQuAD\",                              # Progress bar label\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "28c9b1de06324ca282bb7e4a24b73da7",
            "f8c65de4f5cb42878e3cc08795a27108",
            "5a93c5d47d8944378e5d2af1f5093fc2",
            "3ac1a57221cc47e4bc7b52b92e32f219",
            "cb634eef7cd34c178722034a4a1911cb",
            "92fed82e627b4b139ef33c7c5ab65f2f",
            "93169b9e2e4645d9b9a1d28340cacecb",
            "75e3a828af4c4c95aef8b7a7bb957ad1",
            "abbb4bb357f54d62ac18ac8aedad0274",
            "ed4671525e914f65a62e7fc033966962",
            "1d7b1302194c4831a06895884b6d6f1a"
          ]
        },
        "collapsed": true,
        "id": "BFvsj16TFa-F",
        "outputId": "680698df-2226-4377-b868-3b6976510fe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing SQuAD:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28c9b1de06324ca282bb7e4a24b73da7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Columns required for training with PyTorch-compatible format\n",
        "columns_to_keep = ['input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions']\n",
        "\n",
        "# Set format for training set to return PyTorch tensors\n",
        "tokenized_datasets[\"train\"].set_format(\"torch\", columns=columns_to_keep)\n",
        "\n",
        "# Set format for validation set to return native Python objects (needed for postprocessing with offset_mapping)\n",
        "tokenized_datasets[\"validation\"] = tokenized_datasets[\"validation\"].with_format(\"python\")\n",
        "\n",
        "# Full datasets used for final training and evaluation\n",
        "full_train = tokenized_datasets[\"train\"]\n",
        "full_val = tokenized_datasets[\"validation\"]\n",
        "\n",
        "# Subsets for Experimentation\n",
        "\n",
        "# Very small subset (for debugging or rapid testing)\n",
        "small_train_dataset = tokenized_datasets[\"train\"].select(range(500))\n",
        "small_eval_dataset = tokenized_datasets[\"validation\"].select(range(100))\n",
        "\n",
        "# Medium-sized subset (for tuning hyperparameters before scaling up)\n",
        "mid_train_dataset = tokenized_datasets[\"train\"].select(range(2000))\n",
        "mid_eval_dataset = tokenized_datasets[\"validation\"].select(range(500))\n",
        "\n",
        "# Large subset (near full scale; good for final training before full set)\n",
        "larger_train_dataset = tokenized_datasets[\"train\"].select(range(10000))\n",
        "larger_eval_dataset = tokenized_datasets[\"validation\"].select(range(2000))\n"
      ],
      "metadata": {
        "id": "SC4Sz8bdxqMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BERT QA Training Pipeline\n",
        "\n",
        "# Hugging Face imports\n",
        "from transformers import (\n",
        "    BertForQuestionAnswering,\n",
        "    BertTokenizerFast,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import string\n",
        "\n",
        "squad_metric = evaluate.load(\"squad\")\n",
        "\n",
        "# Compute Metrics Callback: Converts model predictions to answer strings, then calculates EM/F1\n",
        "def compute_metrics(p):\n",
        "    predictions = postprocess_qa_predictions(\n",
        "        examples=dataset[\"validation\"],                   # Raw examples (original context and questions)\n",
        "        features=tokenized_datasets[\"validation\"],        # Tokenized features (with chunking)\n",
        "        raw_predictions=p.predictions,                    # Model outputs: (start_logits, end_logits)\n",
        "        tokenizer=tokenizer,\n",
        "        n_best_size=20,\n",
        "        max_answer_length=30\n",
        "    )\n",
        "\n",
        "    formatted_predictions = [\n",
        "        {\"id\": k, \"prediction_text\": v} for k, v in predictions.items()\n",
        "    ]\n",
        "    references = [\n",
        "        {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in dataset[\"validation\"]\n",
        "    ]\n",
        "\n",
        "    return squad_metric.compute(predictions=formatted_predictions, references=references)\n",
        "\n",
        "# Custom Postprocessing Function: Converts start/end logits into answer spans\n",
        "def postprocess_qa_predictions(\n",
        "    examples,\n",
        "    features,\n",
        "    raw_predictions,\n",
        "    tokenizer,\n",
        "    n_best_size=20,\n",
        "    max_answer_length=30,\n",
        "    score_threshold=0.0\n",
        "):\n",
        "    import collections\n",
        "    import numpy as np\n",
        "\n",
        "    all_start_logits, all_end_logits = raw_predictions\n",
        "\n",
        "    # Map example ID to features\n",
        "    example_id_to_index = {k[\"id\"]: i for i, k in enumerate(examples)}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        # Use 'example_id' added during tokenization or fall back to 'id'\n",
        "        features_per_example[feature.get(\"example_id\", feature[\"id\"])].append(i)\n",
        "\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    for example in examples:\n",
        "        example_id = example[\"id\"]\n",
        "        context = example[\"context\"]\n",
        "        feature_indices = features_per_example[example_id]\n",
        "\n",
        "        valid_answers = []\n",
        "\n",
        "        for i in feature_indices:\n",
        "            start_logits = all_start_logits[i]\n",
        "            end_logits = all_end_logits[i]\n",
        "            offset_mapping = features[i][\"offset_mapping\"]\n",
        "            input_ids = features[i][\"input_ids\"]\n",
        "\n",
        "            # Top n_best start and end token positions\n",
        "            start_indexes = np.argsort(start_logits)[-1: -n_best_size - 1: -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1: -n_best_size - 1: -1].tolist()\n",
        "\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Skip invalid spans\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                        or end_index < start_index\n",
        "                        or end_index - start_index + 1 > max_answer_length\n",
        "                    ):\n",
        "                        continue\n",
        "\n",
        "                    start_char = offset_mapping[start_index][0]\n",
        "                    end_char = offset_mapping[end_index][1]\n",
        "                    answer_text = context[start_char:end_char]\n",
        "                    score = start_logits[start_index] + end_logits[end_index]\n",
        "\n",
        "                    valid_answers.append({\n",
        "                        \"score\": score,\n",
        "                        \"text\": answer_text.strip()\n",
        "                    })\n",
        "\n",
        "        # Select the best valid answer span\n",
        "        if valid_answers:\n",
        "            best_answer = max(valid_answers, key=lambda x: x[\"score\"])\n",
        "            predictions[example_id] = \"\" if best_answer[\"score\"] < score_threshold else best_answer[\"text\"]\n",
        "        else:\n",
        "            predictions[example_id] = \"\"\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bert_qa_results\",               # Save directory for checkpoints\n",
        "    learning_rate=3e-5,                            # Learning rate\n",
        "    per_device_train_batch_size=64,               # Training batch size per GPU\n",
        "    gradient_accumulation_steps=1,                # No accumulation\n",
        "    evaluation_strategy=\"epoch\",                  # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",                        # Save checkpoints at each epoch\n",
        "    num_train_epochs=5,                           # Max 5 epochs\n",
        "    weight_decay=0.01,                            # Regularization\n",
        "    fp16=True,                                     # Mixed precision for speed\n",
        "    report_to=\"wandb\",                             # Log to Weights & Biases\n",
        "    load_best_model_at_end=True,                   # Automatically keep the best checkpoint\n",
        "    metric_for_best_model=\"f1\",                    # Use F1 as best model criterion\n",
        "    greater_is_better=True,                        # Higher F1 is better\n",
        "    lr_scheduler_type=\"linear\",                    # Linear LR decay\n",
        "    warmup_ratio=0.1,                              # 10% LR warmup\n",
        "    logging_steps=50                               # Log every 50 steps\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,                                   # BERT QA model\n",
        "    args=training_args,                            # Training config\n",
        "    train_dataset=full_train,                      # Full tokenized training set\n",
        "    eval_dataset=full_val,                         # Full tokenized validation set\n",
        "    tokenizer=tokenizer,                           # Tokenizer (for saving + decoding)\n",
        "    compute_metrics=compute_metrics,               # Metrics function (EM + F1)\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)],  # Stop early if no F1 gain\n",
        ")\n"
      ],
      "metadata": {
        "id": "D3XhQ4PCVs2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine-Tune\n",
        "trainer.train()\n",
        "trainer.save_model(\"./bert_qa_results\")\n",
        "\n",
        "results = trainer.evaluate()\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "Z0EaSogsFnuo",
        "outputId": "ec0bf00b-743d-4412-e69e-d966e7bbb443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5536' max='6920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5536/6920 28:48 < 07:12, 3.20 it/s, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Exact Match</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.143800</td>\n",
              "      <td>1.152853</td>\n",
              "      <td>75.099338</td>\n",
              "      <td>83.498410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.896200</td>\n",
              "      <td>1.085218</td>\n",
              "      <td>76.982025</td>\n",
              "      <td>85.007131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.656700</td>\n",
              "      <td>1.131582</td>\n",
              "      <td>77.445601</td>\n",
              "      <td>85.189801</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.507900</td>\n",
              "      <td>1.217927</td>\n",
              "      <td>77.029328</td>\n",
              "      <td>84.975843</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1348' max='1348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1348/1348 00:30]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.1315820217132568, 'eval_exact_match': 77.44560075685904, 'eval_f1': 85.18980079450432, 'eval_runtime': 80.0299, 'eval_samples_per_second': 134.75, 'eval_steps_per_second': 16.844, 'epoch': 4.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Predictions and Display Sample Results\n",
        "\n",
        "# Step 1: Run the trained model on the validation set\n",
        "raw_preds = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "\n",
        "# Step 2: Postprocess model logits to get readable answer spans\n",
        "predictions = postprocess_qa_predictions(\n",
        "    examples=dataset[\"validation\"],                # Raw examples with questions + context\n",
        "    features=tokenized_datasets[\"validation\"],     # Tokenized features (chunked and aligned)\n",
        "    raw_predictions=raw_preds.predictions,         # Tuple of (start_logits, end_logits)\n",
        "    tokenizer=tokenizer,\n",
        "    n_best_size=40,                                # Number of top start/end positions to consider\n",
        "    max_answer_length=30,                          # Maximum allowed length for answer span\n",
        "    score_threshold=3.0                            # Ignore low-confidence spans\n",
        ")\n",
        "\n",
        "# Step 3: View the first 5 prediction results\n",
        "for i in range(5):\n",
        "    example = dataset[\"validation\"][i]                 # Original example\n",
        "    example_id = example[\"id\"]                         # Unique ID to match with prediction\n",
        "    question = example[\"question\"]\n",
        "    context = example[\"context\"]\n",
        "    ground_truth = example[\"answers\"][\"text\"][0]       # First annotated answer\n",
        "    predicted_answer = predictions[example_id]         # Our model's predicted answer\n",
        "\n",
        "    # Get a preview of the context (first 200 characters, single-line)\n",
        "    snippet = context[:200].replace(\"\\n\", \" \")\n",
        "\n",
        "    # Print formatted comparison\n",
        "    print(f\"Example {i + 1}\")\n",
        "    print(f\"Question:     {question}\")\n",
        "    print(f\"Ground Truth: {ground_truth}\")\n",
        "    print(f\"Prediction:   {predicted_answer}\")\n",
        "    print(f\"Context Snippet: {snippet}...\")\n",
        "    print(\"-\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zLTP6S6l_bre",
        "outputId": "0a038cd4-40f5-44d1-8bf5-b073c431bae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        }
      ]
    }
  ]
}